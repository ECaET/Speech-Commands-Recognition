{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "MLprojectCopy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg7j-P8MxN3S",
        "colab_type": "text"
      },
      "source": [
        "# **Machine learning project**\n",
        "\n",
        "## **What did you say? I understood Speech Recognition**\n",
        "\n",
        "### *Natascia Caria, Claudia Cozzolino, Alfredo Petrella*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "Cjbnl3mkuypG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################\n",
        "# Imports\n",
        "##################################################\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import IPython.display as ipd\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "!pip install librosa==0.7.2\n",
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvnAxYiI_lUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################\n",
        "# Params\n",
        "##################################################\n",
        "\n",
        "DATA_BASE_FOLDER = '/kaggle/input/ml-project-speech-recognition-challenge'\n",
        "SAMPLE_RATE = 16000\n",
        "HOP_LEN = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmAgGMJcuypO",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset is a reduced version of the [`TensorFlow Speech Commands Dataset`](https://www.tensorflow.org/datasets/catalog/speech_commands) and contains audio waveforms of the words:\n",
        "- `down`, \n",
        "- `go`, \n",
        "- `left`, \n",
        "- `off`, \n",
        "- `on`, \n",
        "- `right`, \n",
        "- `stop`, \n",
        "- `up`.\n",
        "\n",
        "\n",
        "Train / Validation Split\n",
        "- 1600 train samples, 200 for each class\n",
        "- 109 validation samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "oCSL53pGuypP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################\n",
        "# Load dataset\n",
        "##################################################\n",
        "\n",
        "# Load annotations\n",
        "df_train = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'train.csv'))\n",
        "df_validation = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'validation.csv'))\n",
        "labels = sorted(list(set(df_train['word'].values)))\n",
        "y_train = df_train['word'].map(lambda w: labels.index(w)).values\n",
        "y_validation = df_validation['word'].map(lambda w: labels.index(w)).values\n",
        "\n",
        "# Load audio\n",
        "audio_train = np.load(os.path.join(DATA_BASE_FOLDER, 'train_audio.npy'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDkiFVFtuypS",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction\n",
        "\n",
        "The speech is a time series signal and a well known strategy for extracting a good representation of the raw audio is to mimic the processing of the auditory system of the humans. A well established feature representation for speech is the so called \"log mel-spectrum\". This feature in fact, takes into account how humans perceive both the frequencies and the amplitude of the sound logarithmically. If you want to dig more into this topic [here](https://medium.com/@jonathan_hui/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9) you can find some details. \n",
        "\n",
        "![auditory-system](https://www.researchgate.net/profile/Morteza_Khaleghi_Meybodi/publication/322343133/figure/fig1/AS:581011472093184@1515535337239/Figure-31-Schematic-of-the-auditory-system-with-its-primary-components-including.png)\n",
        "\n",
        "For this project these features are precomputed: for each audio waveform of 1 sec duration, the log mel-spectrum is a bi-dimensional representation (frequency vs time) of shape [128, 32]. Here, we first resize the \"image\" into a [32, 32] matrix and then we flatten the representation into a 32x32 = 1024 vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TuyeR0guypS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Features\n",
        "x_train_raw = np.load(os.path.join(DATA_BASE_FOLDER, 'train_feat.npy'))\n",
        "x_validation_raw = np.load(os.path.join(DATA_BASE_FOLDER, 'validation_feat.npy'))\n",
        "\n",
        "x_test_raw = np.load(os.path.join(DATA_BASE_FOLDER, 'test_feat.npy'))\n",
        "\n",
        "# Plot audio feature\n",
        "idx = 1205\n",
        "time = np.arange(1, SAMPLE_RATE + 1, HOP_LEN) / SAMPLE_RATE\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(f'Mel-Spectrogram of audio: {df_train[\"word\"][idx]}', fontweight='bold')\n",
        "plt.imshow(x_train_raw[idx], aspect='auto', origin='low', cmap='inferno')\n",
        "xticks = plt.xticks()[0].astype(np.int32)\n",
        "plt.xticks(xticks[1:-1], [f'{1000 * t:.0f}' for t in time[xticks[1:-1]]])\n",
        "plt.xlabel('Time [ms]', fontweight='bold')\n",
        "plt.ylabel('Log Mel-Spectogram', fontweight='bold')\n",
        "plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "plt.show()\n",
        "\n",
        "# Play audio\n",
        "ipd.Audio(audio_train[idx], rate=SAMPLE_RATE)\n",
        "\n",
        "x_train_raw[idx].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXKO2AqtuypY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resize the features\n",
        "x_train = []\n",
        "for x_i in x_train_raw:\n",
        "    x_train += [cv2.resize(x_i, (32, 32))]\n",
        "x_train = np.array(x_train)\n",
        "x_validation = []\n",
        "for x_i in x_validation_raw:\n",
        "    x_validation += [cv2.resize(x_i, (32, 32))]\n",
        "x_validation = np.array(x_validation)\n",
        "\n",
        "x_test = [] \n",
        "for x_i in x_test_raw:\n",
        "    x_test += [cv2.resize(x_i, (32, 32))]\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "# Plot audio feature\n",
        "idx = 1205\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.title(f'Mel-Spectrogram of audio: {df_train[\"word\"][idx]}', fontweight='bold')\n",
        "plt.imshow(x_train[idx], aspect='auto', origin='low', cmap='inferno')\n",
        "plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "plt.show()\n",
        "\n",
        "# Play audio\n",
        "ipd.Audio(audio_train[idx], rate=SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EECfXz2wgHwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle data\n",
        "\n",
        "tShuff = list(range(x_train.shape[0]))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(tShuff)\n",
        "\n",
        "vShuff = list(range(x_validation.shape[0]))\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(vShuff)\n",
        "\n",
        "df_train = df_train.iloc[tShuff,:]\n",
        "df_train.index = list(range(x_train.shape[0]))\n",
        "\n",
        "df_validation = df_validation.iloc[vShuff,:]\n",
        "df_validation.index = list(range(x_validation.shape[0]))\n",
        "\n",
        "audio_train = audio_train[tShuff,:]\n",
        "\n",
        "x_train_raw = x_train_raw[tShuff,:]\n",
        "x_train = x_train[tShuff,:,:]\n",
        "y_train = y_train[tShuff]\n",
        "\n",
        "x_validation_raw = x_validation_raw[vShuff,:]\n",
        "x_validation = x_validation[vShuff,:,:]\n",
        "y_validation = y_validation[vShuff]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ywXt_sfavmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Matrix features representation\n",
        "x_train_mat = x_train\n",
        "x_validation_mat = x_validation\n",
        "x_test_mat = x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYkmxI93uype",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten the features\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_validation = x_validation.reshape(x_validation.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "print(f'Features dimension size: {x_train.shape[-1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTD6NoYdiWW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "y_test = np.array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
        "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "                   4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
        "                   2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
        "                   6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
        "                   5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
        "                   3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
        "                   1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I92x_tE33bk2",
        "colab_type": "text"
      },
      "source": [
        "# Import augmented data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6N6J9lR3ieL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "from numpy import load\n",
        "\n",
        "x_train_aug_mat = load(os.path.join(DATA_BASE_FOLDER, 'x_train_aug_mat.npy'))\n",
        "x_train_aug = load(os.path.join(DATA_BASE_FOLDER, 'x_train_aug.npy'))\n",
        "y_train_aug = load(os.path.join(DATA_BASE_FOLDER, 'y_train_aug.npy'))\n",
        "x_train_new = load(os.path.join(DATA_BASE_FOLDER, 'x_train_new.npy'))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBx6l9Rcuypi",
        "colab_type": "text"
      },
      "source": [
        "# Model\n",
        "\n",
        "Here you have to implement a model (or more models, for finding the most accurate) for classification.\n",
        "\n",
        "You can use the [`sklearn`](https://scikit-learn.org/stable/) (or optionally other more advanced frameworks such as [`pytorch`](https://pytorch.org/) or [`tensorflow`](https://www.tensorflow.org/)) package that contains a pool of models already implemented that perform classification. (SVMs, NNs, LR, kNN, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4orIlQ16TcV",
        "colab_type": "text"
      },
      "source": [
        "### Some ideas for multiclass learning\n",
        "\n",
        "* One VS One / One VS All binary classifiers using LR / SVM\n",
        "* decision tree / random forest \n",
        "* Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl8epOPD0OeK",
        "colab_type": "text"
      },
      "source": [
        "### Some ideas for performance improvement\n",
        "\n",
        "* more data $\\rightarrow$ data augmentation\n",
        "\n",
        "* more complex models or ensembles\n",
        "\n",
        "* hyperparameters tuning (eg. vector filter on freq,...)\n",
        "\n",
        "* data transformation / scaling\n",
        "\n",
        "* feature selection / engeeniring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGTef7LKidwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# func to print performance scores\n",
        "\n",
        "def print_results(y_true, y_pred):\n",
        "\n",
        "    confusion = confusion_matrix(y_true, y_pred)\n",
        "    print('Confusion Matrix\\n')\n",
        "    print(confusion)\n",
        "\n",
        "    print('\\nAccuracy: {:.4f}\\n'.format(accuracy_score(y_true, y_pred)))\n",
        "\n",
        "    print('Micro Precision: {:.4f}'.format(precision_score(y_true, y_pred, average='micro')))\n",
        "    print('Micro Recall: {:.4f}'.format(recall_score(y_true, y_pred, average='micro')))\n",
        "    print('Micro F1-score: {:.4f}\\n'.format(f1_score(y_true, y_pred, average='micro')))\n",
        "\n",
        "    print('Macro Precision: {:.4f}'.format(precision_score(y_true, y_pred, average='macro')))\n",
        "    print('Macro Recall: {:.4f}'.format(recall_score(y_true, y_pred, average='macro')))\n",
        "    print('Macro F1-score: {:.4f}\\n'.format(f1_score(y_true, y_pred, average='macro')))\n",
        "\n",
        "    print('Weighted Precision: {:.4f}'.format(precision_score(y_true, y_pred, average='weighted')))\n",
        "    print('Weighted Recall: {:.4f}'.format(recall_score(y_true, y_pred, average='weighted')))\n",
        "    print('Weighted F1-score: {:.4f}'.format(f1_score(y_true, y_pred, average='weighted')))\n",
        "\n",
        "    print('\\nClassification Report\\n')\n",
        "    print(classification_report(y_true, y_pred, target_names=labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxOkYJh6TBWI",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression OVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icwv0GSa4S5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logreg_ovr(X_train, Y_train, X_valid, Y_valid, X_test, Y_test):\n",
        "    clf = LogisticRegression(random_state=0, max_iter=50000 ,multi_class='auto')\n",
        "    \n",
        "    # fit the model\n",
        "    clf.fit(X_train, Y_train)\n",
        "    \n",
        "    # predictions\n",
        "    Y_train_pred=clf.predict(X_train)\n",
        "    Y_valid_pred=clf.predict(X_valid)\n",
        "    Y_test_pred=clf.predict(X_test)\n",
        "    \n",
        "    # print accuracy\n",
        "    print(f'Train Accuracy: {metrics.accuracy_score(Y_train, Y_train_pred):.4f}')\n",
        "    print(f'Valid Accuracy: {metrics.accuracy_score(Y_valid, Y_valid_pred):.4f}')\n",
        "    print(f'Test  Accuracy: {metrics.accuracy_score(Y_test,  Y_test_pred):.4f}')\n",
        "    \n",
        "    return(Y_train_pred, Y_valid_pred, Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tQzEa94CXGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results on original data\n",
        "y_train_pred, y_valid_pred, y_test_pred = logreg_ovr(x_train, y_train, x_validation, y_validation, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5E1yWLgD_p3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results on augmented data\n",
        "#y_train_pred, y_valid_pred, y_test_pred = logreg_ovr(x_train_aug, y_train_aug, x_validation, y_validation, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehWojKnxCffo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print_results(y_train, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaHclzfG0pku",
        "colab_type": "text"
      },
      "source": [
        "### SVM OVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBhGSLvcClGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svm_ovr(X_train, Y_train, X_valid, Y_valid, X_test, Y_test):\n",
        "    clf = make_pipeline(preprocessing.Normalizer(),\n",
        "                        svm.NuSVC(kernel='rbf', decision_function_shape='ovr')\n",
        "                       )\n",
        "    # fit the model\n",
        "    clf.fit(X_train, Y_train)\n",
        "    \n",
        "    # predictions\n",
        "    Y_train_pred=clf.predict(X_train)\n",
        "    Y_valid_pred=clf.predict(X_valid)\n",
        "    Y_test_pred=clf.predict(X_test)\n",
        "    \n",
        "    # print accuracy\n",
        "    print(f'Train Accuracy: {metrics.accuracy_score(Y_train, Y_train_pred):.4f}')\n",
        "    print(f'Valid Accuracy: {metrics.accuracy_score(Y_valid, Y_valid_pred):.4f}')\n",
        "    print(f'Test  Accuracy: {metrics.accuracy_score(Y_test,  Y_test_pred):.4f}')\n",
        "    \n",
        "    return(Y_train_pred, Y_valid_pred, Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOm3fNQ60tgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results on original data\n",
        "y_train_pred, y_valid_pred, y_test_pred = svm_ovr(x_train, y_train, x_validation, y_validation, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxm8LnyZEOz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results on augmented data\n",
        "# y_train_pred, y_valid_pred, y_test_pred = svm_ovr(x_train_aug, y_train_aug, x_validation, y_validation, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BIWTcnbC7af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print_results(y_train, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifPS000MMA-R",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBYETCA6DA48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randfor(X_train, Y_train, X_valid, Y_valid, X_test, Y_test):\n",
        "    clf = RandomForestClassifier(criterion='entropy', random_state=0, max_depth=10)\n",
        "    \n",
        "    # fit the model\n",
        "    clf.fit(X_train, Y_train)\n",
        "    \n",
        "    # predictions\n",
        "    Y_train_pred=clf.predict(X_train)\n",
        "    Y_valid_pred=clf.predict(X_valid)\n",
        "    Y_test_pred=clf.predict(X_test)\n",
        "    \n",
        "    # print accuracy\n",
        "    print(f'Train Accuracy: {metrics.accuracy_score(Y_train, Y_train_pred):.4f}')\n",
        "    print(f'Valid Accuracy: {metrics.accuracy_score(Y_valid, Y_valid_pred):.4f}')\n",
        "    print(f'Test  Accuracy: {metrics.accuracy_score(Y_test,  Y_test_pred):.4f}')\n",
        "    \n",
        "    return(Y_train_pred, Y_valid_pred, Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXbSIseeDNa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results on original data\n",
        "y_train_pred, y_valid_pred, y_test_pred = randfor(x_train, y_train, x_validation, y_validation, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWoCBizPEpMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results on augmented data\n",
        "# y_train_pred, y_valid_pred, y_test_pred = randfor(x_train_aug, y_train_aug, x_validation, y_validation, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkXy6x_ePTzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print_results(y_train, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7M4N51RldJN",
        "colab_type": "text"
      },
      "source": [
        "## Models comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCHfcmNxlvGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LogisticRegressionOVR = LogisticRegression(random_state=0, max_iter=50000 ,multi_class='auto')\n",
        "\n",
        "SVMOVR = make_pipeline(preprocessing.Normalizer(), svm.NuSVC(kernel='rbf', decision_function_shape='ovr'))\n",
        "\n",
        "RandForest = RandomForestClassifier(criterion='entropy', random_state=0, max_depth=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6iXapjvmdqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare models\n",
        "models = []\n",
        "#models.append(('LR', LogisticRegressionOVR))\n",
        "models.append(('SVM', SVMOVR))\n",
        "models.append(('RF', RandForest))\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "def model_comparison(models, k, X, Y):\n",
        "    for name, model in models:\n",
        "        skfold = StratifiedKFold(n_splits=k, random_state=1234, shuffle=True)\n",
        "        cv_results = cross_val_score(model, X, Y, cv=skfold, scoring = 'accuracy')\n",
        "        results.append(cv_results)\n",
        "        names.append(name)\n",
        "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "        print(msg)\n",
        "\n",
        "    # boxplot algorithm comparison\n",
        "    fig = plt.figure()\n",
        "    fig.suptitle('Models Comparison')\n",
        "    ax = fig.add_subplot(111)\n",
        "    plt.boxplot(results)\n",
        "    ax.set_xticklabels(names)\n",
        "    plt.show()\n",
        "    plt.savefig('cv_models.pdf', bbox_inches='tight')\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZROwG7Zmmbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results on original data\n",
        "model_comparison(models, 5, x, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFkSOlmAG2CR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results on augmented data\n",
        "# model_comparison(models, 5, x_train_aug, y_train_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW2JHjsmX2TQ",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VYkrHwXYv7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(history):\n",
        "  plt.figure(figsize=(8,5))\n",
        "  plt.plot(history.epoch,history.history['loss'], label = 'loss')\n",
        "  plt.plot(history.epoch,history.history['val_loss'], label = 'val_loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.title('loss')\n",
        "\n",
        "\n",
        "def plot_accuracy(history):\n",
        "  plt.figure(figsize=(8,5))\n",
        "  plt.plot(history.epoch,history.history['accuracy'], label = 'accuracy')\n",
        "  plt.plot(history.epoch,history.history['val_accuracy'], label = 'val_accuracy')\n",
        "  plt.legend(loc = 'lower right')\n",
        "  plt.grid(True)\n",
        "  plt.title('accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSIDNPCrVi4f",
        "colab_type": "text"
      },
      "source": [
        "**Input reshape for 1D NN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alloXWlkQYg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape input (original data)\n",
        "x_train_1D = x_train_mat.reshape(x_train_mat.shape[0], 1024, 1)\n",
        "x_validation_1D = x_validation_mat.reshape(x_validation_mat.shape[0], 1024,1)\n",
        "x_test_1D = x_test_mat.reshape(x_test_mat.shape[0], 1024,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "050wlJBkQo00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape input (augmented data)\n",
        "# x_train_1D_aug = x_train_aug.reshape(x_train_aug.shape[0], 1024, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hxNrRVcVqUz",
        "colab_type": "text"
      },
      "source": [
        "**Input reshape for 2D NN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd2SiMIJVv46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape input\n",
        "x_train_2D = x_train.reshape(x_train.shape[0], 32, 32, 1)\n",
        "x_validation_2D = x_validation.reshape(x_validation.shape[0], 32, 32, 1)\n",
        "x_test_2D = x_test.reshape(x_test.shape[0], 32, 32, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jandOUF7WKea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape input (augmented data)\n",
        "# x_train_2D_aug = x_train_aug.reshape(x_train_aug.shape[0], 32, 32, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOhiX8_W9Hb6",
        "colab_type": "text"
      },
      "source": [
        "### DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGwIyfbBBnXQ",
        "colab_type": "text"
      },
      "source": [
        "#### Hyper parameters tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15MDbHuU9KsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a function that creates the model (required for KerasClassifier) \n",
        "# Input: the hyperparameters we want to tune \n",
        "# Output: DNN model\n",
        "\n",
        "def create_dnn(n_units = 64):\n",
        "    # define model\n",
        "    dnn_model = keras.models.Sequential([\n",
        "        keras.layers.Flatten(input_shape=[1024, 1]),\n",
        "        keras.layers.Dense(n_units*4, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        keras.layers.Dense(n_units*4, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        keras.layers.Dense(n_units*2, activation=\"relu\",  kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        keras.layers.Dense(n_units*2, activation=\"relu\",  kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        keras.layers.Dense(n_units, activation=\"relu\",  kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "        keras.layers.Dense(8, activation=\"softmax\")\n",
        "    ])\n",
        "    # compile model\n",
        "    dnn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer='adam',\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return dnn_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI0D4u8q-OHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a keras.wrappers.scikit_learn.KerasRegressor and pass the build_model function to the constructor\n",
        "# this gives a Scikit-Learn compatible predictor\n",
        "keras_reg_dnn = keras.wrappers.scikit_learn.KerasRegressor(create_dnn)\n",
        "\n",
        "# define the grid search parameters\n",
        "params_dnn = {\n",
        "    \"n_units\": [32, 64, 128],\n",
        "    \"batch_size\": [128, 256],\n",
        "    \"epochs\": [50, 100]\n",
        "}\n",
        "\n",
        "\n",
        "# define accuracy score in order to avoid error:\n",
        "# \"Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets\"\n",
        "def acc(y_true, y_pred):\n",
        "    score = accuracy_score(np.argmax(y_pred, axis = 1), y_true)\n",
        "    print('score is {}'.format(score))\n",
        "    return score\n",
        "acc_score = make_scorer(acc)\n",
        "\n",
        "\n",
        "# define StratifiedKFold CV to be used within GridSearch\n",
        "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
        "\n",
        "grid_search = GridSearchCV(keras_reg_dnn, params_dnn, cv=inner_cv, scoring=acc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT8i9Z3X_N2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid search\n",
        "# uncomment next lines to run\n",
        "'''\n",
        "grid_result = grid_search.fit(x_train_1D, y_train, verbose=False)\n",
        "\n",
        "# print results\n",
        "print(f'\\nBest Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5BzRERpAfyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# evaluate the best model training on entire training set and validating on validation set\n",
        "model = grid_search.best_estimator_.model\n",
        "\n",
        "#model.evaluate(x_train_1D, y_train)\n",
        "#model.evaluate(x_validation_1D, y_validation)\n",
        "#model.evaluate(x_test_1D, y_test)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFJx4migBdVm",
        "colab_type": "text"
      },
      "source": [
        "#### Best model and early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WaV3NvsA3gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dnn(X_train, Y_train, X_valid, Y_valid, X_test, Y_test,\n",
        "        n_units, epochs, batch_size):\n",
        "\n",
        "    model_dnn = create_dnn(n_units)\n",
        "\n",
        "    # define a log dir in order to save the checkpoint file\n",
        "    logdir = os.path.join(os.curdir, \"my_logs\")\n",
        "\n",
        "    # in Keras the Eraly stopping is manage by using the callbacks argument.\n",
        "    callbacks_dnn = [\n",
        "        keras.callbacks.TensorBoard(logdir),\n",
        "        keras.callbacks.EarlyStopping(patience=5),\n",
        "        #Saving the checkpoints file allows to load the \"best\" model when the Early \n",
        "        #stopping detect that the generalization error degrade (after 'patience' epochs)\n",
        "        keras.callbacks.ModelCheckpoint(\"my_model_dnn.h5\", save_best_only=True),\n",
        "    ]\n",
        "\n",
        "\n",
        "    history = model_dnn.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
        "                        validation_data=(X_valid, Y_valid),\n",
        "                        callbacks=callbacks_dnn,\n",
        "                        verbose = 1)\n",
        "\n",
        "    #model_dnn = keras.models.load_model(\"my_model_dnn.h5\")\n",
        "\n",
        "    plot_loss(history)\n",
        "    plot_accuracy(history)\n",
        "\n",
        "    Y_train_pred = np.argmax(model_dnn.predict(X_train), axis=1)\n",
        "    Y_valid_pred = np.argmax(model_dnn.predict(X_valid), axis=1)\n",
        "    Y_test_pred = np.argmax(model_dnn.predict(X_test), axis=1)\n",
        "    \n",
        "    # print accuracy\n",
        "    print(f'Train Accuracy: {accuracy_score(Y_train, Y_train_pred):.4f}')\n",
        "    print(f'Valid Accuracy: {accuracy_score(Y_valid, Y_valid_pred):.4f}')\n",
        "    print(f'Test  Accuracy: {accuracy_score(Y_test,  Y_test_pred):.4f}')\n",
        "    \n",
        "    return (Y_train_pred, Y_valid_pred, Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOR5i5qHJiYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model\n",
        "n_units = 64\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = dnn(x_train_1D, y_train,\n",
        "                                              x_validation_1D, y_validation,\n",
        "                                              x_test_1D, y_test,\n",
        "                                              n_units, epochs, batch_size)\n",
        "\n",
        "# print_results(y_train, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x1DPggqVMWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model on augmented data\n",
        "\n",
        "'''\n",
        "n_units = 64\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = dnn(x_train_1D_aug, y_train_aug,\n",
        "                                              x_validation_1D, y_validation,\n",
        "                                              x_test_1D, y_test,\n",
        "                                              n_units, epochs, batch_size)\n",
        "\n",
        "\n",
        "# print_results(y_train_aug, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDGbw9KvZqCM",
        "colab_type": "text"
      },
      "source": [
        "### CNN 1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuIyRypjDk7C",
        "colab_type": "text"
      },
      "source": [
        "##### Hyper parameters tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Im-knq-DtRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a function that creates the model (required for KerasClassifier) \n",
        "# Input: the hyperparameters we want to tune \n",
        "# Output: CNN model\n",
        "\n",
        "def create_cnn1(filters = 32, kernel_size = 3, pool_size=2):\n",
        "    # define model\n",
        "    cnn1_model = keras.models.Sequential([\n",
        "        keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=\"relu\", input_shape=[1024, 1]),\n",
        "        keras.layers.MaxPool1D(pool_size=pool_size),\n",
        "        keras.layers.Conv1D(filters=2*filters, kernel_size=kernel_size, padding=\"same\", activation=\"relu\"),\n",
        "        keras.layers.MaxPool1D(pool_size=pool_size),\n",
        "        keras.layers.Conv1D(filters=4*filters, kernel_size=kernel_size, padding=\"same\", activation=\"relu\"),\n",
        "        keras.layers.MaxPool1D(pool_size=pool_size),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(8, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    # compile model\n",
        "    cnn1_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer='adam',\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return cnn1_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66XaXT99EHI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a keras.wrappers.scikit_learn.KerasRegressor and pass the build_model function to the constructor\n",
        "# this gives a Scikit-Learn compatible predictor\n",
        "keras_reg_cnn1 = keras.wrappers.scikit_learn.KerasRegressor(create_cnn1)\n",
        "\n",
        "# define the grid search parameters\n",
        "params_cnn1 = {\n",
        "    \"filters\": [32, 64],\n",
        "    \"kernel_size\": [3,5],\n",
        "    \"pool_size\": [2,3],\n",
        "    \"batch_size\": [128, 256],\n",
        "    \"epochs\": [30]\n",
        "}\n",
        "\n",
        "# define accuracy score in order to avoid error:\n",
        "# \"Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets\"\n",
        "def acc(y_true, y_pred):\n",
        "    score = accuracy_score(np.argmax(y_pred, axis = 1), y_true)\n",
        "    print('score is {}'.format(score))\n",
        "    return score\n",
        "acc_score = make_scorer(acc)\n",
        "\n",
        "\n",
        "# define StratifiedKFold to be used within GridSearch\n",
        "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
        "\n",
        "grid_search = GridSearchCV(keras_reg_cnn1, params_cnn1, cv=inner_cv, scoring=acc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcVyaJzsE9Y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid search\n",
        "# uncomment next lines to run\n",
        "'''\n",
        "grid_result = grid_search.fit(x_train_1D, y_train, epochs=10, verbose=False)\n",
        "\n",
        "# print results\n",
        "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty5kEOCOGlz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# evaluate the best model training on entire training set and validating on validation set\n",
        "model = grid_search.best_estimator_.model\n",
        "\n",
        "#model.evaluate(x_train_1D, y_train)\n",
        "#model.evaluate(x_validation_1D, y_validation)\n",
        "#model.evaluate(x_test_1D, y_test)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBJoTqbCHNS1",
        "colab_type": "text"
      },
      "source": [
        "##### Best model and early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smG_rKfEX0aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn1(X_train, Y_train, X_valid, Y_valid, X_test, Y_test,\n",
        "         filters, kernel_size, pool_size, epochs, batch_size):\n",
        "\n",
        "    model_cnn1 = create_cnn1(filters, kernel_size, pool_size)\n",
        "\n",
        "    # define a log dir in order to save the checkpoint file\n",
        "    logdir = os.path.join(os.curdir, \"my_logs\")\n",
        "\n",
        "    # in Keras the Eraly stopping is manage by using the callbacks argument.\n",
        "    callbacks_cnn1 = [\n",
        "        keras.callbacks.TensorBoard(logdir),\n",
        "        keras.callbacks.EarlyStopping(patience=5),\n",
        "        #Saving the checkpoints file allows to load the \"best\" model when the Early \n",
        "        #stopping detect that the generalization error degrade (after 'patience' epochs)\n",
        "        keras.callbacks.ModelCheckpoint(\"my_model_cnn1.h5\", save_best_only=True),\n",
        "    ]\n",
        "\n",
        "    history = model_cnn1.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
        "                        validation_data=(X_valid, Y_valid),\n",
        "                        callbacks=callbacks_cnn1,\n",
        "                        verbose = 1)\n",
        "\n",
        "    #model_cnn1 = keras.models.load_model(\"my_model_cnn1.h5\")\n",
        "\n",
        "    plot_loss(history)\n",
        "    plot_accuracy(history)\n",
        "\n",
        "    Y_train_pred = np.argmax(model_cnn1.predict(X_train), axis=1)\n",
        "    Y_valid_pred = np.argmax(model_cnn1.predict(X_valid), axis=1)\n",
        "    Y_test_pred = np.argmax(model_cnn1.predict(X_test), axis=1)\n",
        "    \n",
        "    # print accuracy\n",
        "    print(f'Train Accuracy: {accuracy_score(Y_train, Y_train_pred):.4f}')\n",
        "    print(f'Valid Accuracy: {accuracy_score(Y_valid, Y_valid_pred):.4f}')\n",
        "    print(f'Test  Accuracy: {accuracy_score(Y_test,  Y_test_pred):.4f}')\n",
        "    \n",
        "    return (Y_train_pred, Y_valid_pred, Y_test_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpXa2a4jUbWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model\n",
        "filters = 32\n",
        "kernel_size = 5\n",
        "pool_size = 3\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = cnn1(x_train_1D, y_train,\n",
        "                                               x_validation_1D, y_validation,\n",
        "                                               x_test_1D, y_test,\n",
        "                                               filters, kernel_size, pool_size,\n",
        "                                               epochs, batch_size)\n",
        "\n",
        "# print_results(y_train, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQN0MpRBUgv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using same hyperparameter with augmented dataset\n",
        "'''\n",
        "\n",
        "filters = 32\n",
        "kernel_size = 5\n",
        "pool_size = 3\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = cnn1(x_train_1D_aug, y_train_aug,\n",
        "                                               x_validation_1D, y_validation,\n",
        "                                               x_test_1D, y_test,\n",
        "                                               filters, kernel_size, pool_size,\n",
        "                                               epochs, batch_size)\n",
        "\n",
        "# print_results(y_train_aug, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oPncGcdZxV-",
        "colab_type": "text"
      },
      "source": [
        "### CNN 2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGDQRAaK4vr",
        "colab_type": "text"
      },
      "source": [
        "##### Hyper parameters tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auXnTIo0K9T9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a function that creates the model (required for KerasClassifier) \n",
        "# Input: the hyperparameters we want to tune \n",
        "# Output: CNN model\n",
        "\n",
        "def create_cnn2(filters = 32, kernel_size = [3,3], pool_size=[2,2]):\n",
        "    # define model\n",
        "    model_cnn2 = keras.models.Sequential([\n",
        "        keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=\"relu\",\n",
        "                            kernel_regularizer = tf.keras.regularizers.l2(0.01), input_shape=[32, 32,1]),\n",
        "        keras.layers.MaxPool2D(pool_size=pool_size),\n",
        "        keras.layers.Conv2D(filters=filters*2, kernel_size=kernel_size, padding=\"same\", activation=\"relu\",\n",
        "                           kernel_regularizer = tf.keras.regularizers.l2(0.01),),\n",
        "        keras.layers.MaxPool2D(pool_size=pool_size),\n",
        "        keras.layers.Conv2D(filters=filters*4, kernel_size=kernel_size, padding=\"same\", activation=\"relu\"),\n",
        "        keras.layers.MaxPool2D(pool_size=pool_size),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(8, activation=\"softmax\")\n",
        "    ])\n",
        "    \n",
        "    model_cnn2.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                  optimizer='adam',\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "    return model_cnn2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw1R3WU0Lb9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a keras.wrappers.scikit_learn.KerasRegressor and pass the build_model function to the constructor\n",
        "# this gives a Scikit-Learn compatible predictor\n",
        "keras_reg_cnn2 = keras.wrappers.scikit_learn.KerasRegressor(create_cnn2)\n",
        "\n",
        "# define the grid search parameters\n",
        "params_cnn2 = {\n",
        "    \"filters\": [32, 64],\n",
        "    \"kernel_size\": [3, 5],\n",
        "    \"pool_size\": [2, 3],\n",
        "    \"batch_size\": [128, 256],\n",
        "    \"epochs\": [30]\n",
        "}\n",
        "\n",
        "\n",
        "# define accuracy score in order to avoid error:\n",
        "# \"Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets\"\n",
        "def acc(y_true, y_pred):\n",
        "    score = accuracy_score(np.argmax(y_pred, axis = 1), y_true)\n",
        "    print('score is {}'.format(score))\n",
        "    return score\n",
        "acc_score = make_scorer(acc)\n",
        "\n",
        "\n",
        "# define StratifiedKFold CV to be used within GridSearch\n",
        "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
        "\n",
        "grid_search = GridSearchCV(keras_reg_cnn2, params_cnn2, cv=inner_cv, scoring=acc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymFPtREVMJ8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid search\n",
        "# uncomment next lines to run\n",
        "'''\n",
        "grid_result = grid_search.fit(x_train_2D, y_train, epochs=40, verbose=False)\n",
        "\n",
        "# print results\n",
        "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l17DuIFhMo5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# evaluate the best model training on entire training set and validating on validation set\n",
        "model = grid_search.best_estimator_.model\n",
        "\n",
        "#model.evaluate(x_train_1D, y_train)\n",
        "#model.evaluate(x_validation_1D, y_validation)\n",
        "#model.evaluate(x_test_1D, y_test)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39n1w1xQMxsa",
        "colab_type": "text"
      },
      "source": [
        "##### Best model and early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DlpMlFhNMM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cnn2(X_train, Y_train, X_valid, Y_valid, X_test, Y_test,\n",
        "         filters, kernel_size, pool_size, epochs, batch_size):\n",
        "\n",
        "    model_cnn2 = create_cnn2(filters, kernel_size, pool_size)\n",
        "\n",
        "    # define a log dir in order to save the checkpoint file\n",
        "    logdir = os.path.join(os.curdir, \"my_logs\")\n",
        "\n",
        "    # in Keras the Eraly stopping is manage by using the callbacks argument.\n",
        "    callbacks_cnn2 = [\n",
        "        keras.callbacks.TensorBoard(logdir),\n",
        "        keras.callbacks.EarlyStopping(patience=5),\n",
        "        #Saving the checkpoints file allows to load the \"best\" model when the Early \n",
        "        #stopping detect that the generalization error degrade (after 'patience' epochs)\n",
        "        keras.callbacks.ModelCheckpoint(\"my_model_cnn2.h5\", save_best_only=True),\n",
        "    ]\n",
        "\n",
        "    history = model_cnn2.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
        "                        validation_data=(X_valid, Y_valid),\n",
        "                        callbacks=callbacks_cnn2,\n",
        "                        verbose = 1)\n",
        "\n",
        "    model_cnn2 = keras.models.load_model(\"my_model_cnn2.h5\")\n",
        "\n",
        "    plot_loss(history)\n",
        "    plot_accuracy(history)\n",
        "\n",
        "    Y_train_pred = np.argmax(model_cnn2.predict(X_train), axis=1)\n",
        "    Y_valid_pred = np.argmax(model_cnn2.predict(X_valid), axis=1)\n",
        "    Y_test_pred = np.argmax(model_cnn2.predict(X_test), axis=1)\n",
        "    \n",
        "    # print accuracy\n",
        "    print(f'Train Accuracy: {accuracy_score(Y_train, Y_train_pred):.4f}')\n",
        "    print(f'Valid Accuracy: {accuracy_score(Y_valid, Y_valid_pred):.4f}')\n",
        "    print(f'Test  Accuracy: {accuracy_score(Y_test,  Y_test_pred):.4f}')\n",
        "    \n",
        "    return (Y_train_pred, Y_valid_pred, Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "376ODlc7ZmbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model\n",
        "filters = 64\n",
        "kernel_size = 5\n",
        "pool_size = 2\n",
        "epochs = 30\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = cnn2(x_train_2D, y_train,\n",
        "                                               x_validation_2D, y_validation,\n",
        "                                               x_test_2D, y_test,\n",
        "                                               filters, kernel_size, pool_size,\n",
        "                                               epochs, batch_size)\n",
        "\n",
        "# print_results(y_train, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BiOWGYvZlU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model on augmented data\n",
        "\n",
        "'''\n",
        "filters = 64\n",
        "kernel_size = 3\n",
        "pool_size = 2\n",
        "epochs = 30\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = cnn2(x_train_2D_aug, y_train_aug,\n",
        "                                               x_validation_2D, y_validation,\n",
        "                                               x_test_2D, y_test,\n",
        "                                               filters, kernel_size, pool_size,\n",
        "                                               epochs, batch_size)\n",
        "\n",
        "# print_results(y_train_aug, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yM5CgJQfYWPN"
      },
      "source": [
        "### CRNN 1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV4o3-3dv4O9",
        "colab_type": "text"
      },
      "source": [
        "##### Hyper parameters tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSu9HGR8v4O-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a function that creates the model (required for KerasClassifier) \n",
        "# Input: the hyperparameters we want to tune \n",
        "# Output: RNN model\n",
        "\n",
        "def create_rnn1(filters = 32, kernel_size = 3, pool_size=2, GRUunits = 32):\n",
        "    # define model\n",
        "    model_rnn1 = keras.models.Sequential([\n",
        "        keras.layers.Conv1D(filters=filters, kernel_size= kernel_size, padding=\"same\", activation=\"relu\", input_shape=[1024, 1]),\n",
        "        keras.layers.MaxPool1D(pool_size= pool_size),\n",
        "        keras.layers.Conv1D(filters=filters*2, kernel_size= kernel_size, padding=\"same\", activation=\"relu\"),\n",
        "        keras.layers.MaxPool1D(pool_size= pool_size),\n",
        "        keras.layers.Conv1D(filters=filters*4, kernel_size= kernel_size, padding=\"same\", activation=\"relu\"),\n",
        "        keras.layers.MaxPool1D(pool_size= pool_size),\n",
        "        #keras.layers.Dropout(0.8),\n",
        "        #keras.layers.Bidirectional(keras.layers.LSTM(16)),\n",
        "        keras.layers.GRU(GRUunits),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(8, activation=\"softmax\")])\n",
        "    model_rnn1.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer='adam',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "    return model_rnn1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLhG9NTToLif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a keras.wrappers.scikit_learn.KerasRegressor and pass the build_model function to the constructor\n",
        "# this gives a Scikit-Learn compatible predictor\n",
        "keras_reg_rnn1 = keras.wrappers.scikit_learn.KerasRegressor(create_rnn1)\n",
        "\n",
        "# define the grid search parameters\n",
        "params_rnn1 = {\n",
        "    \"filters\": [32,64],\n",
        "    \"kernel_size\": [3,5],\n",
        "    \"pool_size\": [2,3],\n",
        "    \"GRUunits\":[64],\n",
        "    \"batch_size\":[128,256],\n",
        "    \"epochs\":[30]},\n",
        "\n",
        "\n",
        "# define accuracy score in order to avoid error:\n",
        "# \"Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets\"\n",
        "def acc(y_true, y_pred):\n",
        "    score = accuracy_score(np.argmax(y_pred, axis = 1), y_true)\n",
        "    print('score is {}'.format(score))\n",
        "    return score\n",
        "acc_score = make_scorer(acc)\n",
        " \n",
        "# define StratifiedKFold to be used within GridSearch\n",
        "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(keras_reg_rnn1, params_rnn1, cv=inner_cv, scoring=acc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wnG4fWg7RmBH",
        "colab": {}
      },
      "source": [
        "# grid search\n",
        "# uncomment next lines to run\n",
        "\n",
        "'''\n",
        "grid_result = grid_search.fit(x_train_1D, y_train, epochs=40, verbose=False)\n",
        "\n",
        "# print results\n",
        "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kxq4gnnZRmBb",
        "colab": {}
      },
      "source": [
        "'''\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# evaluate the best model training on entire training set and validating on validation set\n",
        "model = grid_search.best_estimator_.model\n",
        "\n",
        "#model.evaluate(x_train_1D, y_train)\n",
        "#model.evaluate(x_validation_1D, y_validation)\n",
        "#model.evaluate(x_test_1D, y_test)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8qOttfVv4Pi",
        "colab_type": "text"
      },
      "source": [
        "##### Best model and early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YuFOug21sHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn1(X_train, Y_train, X_valid, Y_valid, X_test, Y_test,\n",
        "         filters, kernel_size, pool_size, GRUunits, epochs, batch_size):\n",
        "\n",
        "    model_rnn1 = create_rnn1(filters, kernel_size, pool_size, GRUunits)\n",
        "\n",
        "    # define a log dir in order to save the checkpoint file\n",
        "    logdir = os.path.join(os.curdir, \"my_logs\")\n",
        "\n",
        "    # in Keras the Eraly stopping is manage by using the callbacks argument.\n",
        "    callbacks_rnn1 = [\n",
        "        keras.callbacks.TensorBoard(logdir),\n",
        "        keras.callbacks.EarlyStopping(patience=5),\n",
        "        #Saving the checkpoints file allows to load the \"best\" model when the Early \n",
        "        #stopping detect that the generalization error degrade (after 'patience' epochs)\n",
        "        keras.callbacks.ModelCheckpoint(\"my_model_rnn1.h5\", save_best_only=True),\n",
        "    ]\n",
        "\n",
        "    history = model_rnn1.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
        "                        validation_data=(X_valid, Y_valid),\n",
        "                        callbacks=callbacks_rnn1,\n",
        "                        verbose = 1)\n",
        "\n",
        "    #model_rnn1 = keras.models.load_model(\"my_model_rnn1.h5\")\n",
        "\n",
        "    plot_loss(history)\n",
        "    plot_accuracy(history)\n",
        "\n",
        "    Y_train_pred = np.argmax(model_rnn1.predict(X_train), axis=1)\n",
        "    Y_valid_pred = np.argmax(model_rnn1.predict(X_valid), axis=1)\n",
        "    Y_test_pred = np.argmax(model_rnn1.predict(X_test), axis=1)\n",
        "    \n",
        "    # print accuracy\n",
        "    print(f'Train Accuracy: {accuracy_score(Y_train, Y_train_pred):.4f}')\n",
        "    print(f'Valid Accuracy: {accuracy_score(Y_valid, Y_valid_pred):.4f}')\n",
        "    print(f'Test  Accuracy: {accuracy_score(Y_test,  Y_test_pred):.4f}')\n",
        "    \n",
        "    return (Y_train_pred, Y_valid_pred, Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcsn6u1q29v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model\n",
        "filters=64\n",
        "kernel_size=5\n",
        "pool_size=3\n",
        "GRUunits=64\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = rnn1(x_train_1D, y_train,\n",
        "                                               x_validation_1D, y_validation,\n",
        "                                               x_test_1D, y_test,\n",
        "                                               filters, kernel_size, pool_size,\n",
        "                                               GRUunits, epochs, batch_size)\n",
        "\n",
        "# print_results(y_train, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5RRXAaCEpHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using same hyperparameters on augmented data\n",
        "\n",
        "'''\n",
        "filters=64\n",
        "kernel_size=5\n",
        "pool_size=3\n",
        "GRUunits=64\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = rnn1(x_train_1D_aug, y_train_aug,\n",
        "                                               x_validation_1D, y_validation,\n",
        "                                               x_test_1D, y_test,\n",
        "                                               filters, kernel_size, pool_size,\n",
        "                                               GRUunits, epochs, batch_size)\n",
        "\n",
        "# print_results(y_train_aug, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXVs4TtuxvH0",
        "colab_type": "text"
      },
      "source": [
        "### CRNN 2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ajTWgtZxyRX",
        "colab_type": "text"
      },
      "source": [
        "##### Hyper parameters tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TepINObDv4Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a function that creates the model (required for KerasClassifier) \n",
        "# Input: the hyperparameters we want to tune \n",
        "# Output: RNN model\n",
        "\n",
        "def create_rnn2(filters = 32, kernel_size = 3, pool_size=2, GRUunits=32):\n",
        "    \n",
        "    # define model\n",
        "    model_rnn2 = keras.models.Sequential([\n",
        "        keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=\"relu\",\n",
        "                        kernel_regularizer = tf.keras.regularizers.l2(0.01), input_shape=[32,32,1]),\n",
        "        #keras.layers.BatchNormalization(),\n",
        "        keras.layers.MaxPool2D(pool_size=pool_size),\n",
        "        keras.layers.Conv2D(filters=filters*2, kernel_size=kernel_size, padding=\"same\", activation=\"relu\",\n",
        "                        kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n",
        "        #keras.layers.BatchNormalization(),\n",
        "        keras.layers.MaxPool2D(pool_size=pool_size),\n",
        "        keras.layers.Conv2D(filters=filters*4, kernel_size=kernel_size, padding=\"same\", activation=\"relu\",\n",
        "                        kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n",
        "        #keras.layers.BatchNormalization(),\n",
        "        keras.layers.MaxPool2D(pool_size=pool_size),\n",
        "        keras.layers.Reshape((np.floor(x_train_mat.shape[1]/pool_size**3)**2, filters*4)), # depends on the kernel size!\n",
        "        keras.layers.GRU(GRUunits),\n",
        "        #keras.layers.Flatten(),\n",
        "        keras.layers.Dense(32, activation=\"relu\"), \n",
        "        keras.layers.Dense(8, activation=\"softmax\")])\n",
        "    \n",
        "    model_rnn2.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer='adam',\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "    return model_rnn2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTGsr6THyYKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a keras.wrappers.scikit_learn.KerasRegressor and pass the build_model function to the constructor\n",
        "# this gives a Scikit-Learn compatible predictor\n",
        "keras_reg_rnn2 = keras.wrappers.scikit_learn.KerasRegressor(create_rnn2)\n",
        "# define the grid search parameters\n",
        "\n",
        "params_rnn2 = {\n",
        "    \"filters\": [16, 32, 64],\n",
        "    \"kernel_size\": [3,5],\n",
        "    \"pool_size\": [2,3],\n",
        "    \"GRUunits\":[16, 32],\n",
        "    \"batch_size\":[128, 256],\n",
        "    \"epochs\":[30]}\n",
        "\n",
        "\n",
        "# define accuracy score in order to avoid error:\n",
        "# \"Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets\"\n",
        "def acc(y_true, y_pred):\n",
        "    score = accuracy_score(np.argmax(y_pred, axis = 1), y_true)\n",
        "    print('score is {}'.format(score))\n",
        "    return score\n",
        "acc_score = make_scorer(acc)\n",
        "\n",
        "\n",
        "# define StratifiedKFold CV to be used within GridSearch\n",
        "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
        " \n",
        "grid_search = GridSearchCV(keras_reg_rnn2, params_rnn2, cv=inner_cv, scoring=acc_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54sEmo3M0wFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grid search\n",
        "# uncomment next lines to run\n",
        "'''\n",
        "grid_result = grid_search.fit(x_train_2D, y_train, epochs=40, verbose=False)\n",
        "\n",
        "# print results\n",
        "print(f'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_}')\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f' mean={mean:.4}, std={stdev:.4} using {param}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-7jbSK80xY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# evaluate the best model training on entire training set and validating on validation set\n",
        "model = grid_search.best_estimator_.model\n",
        "\n",
        "#model.evaluate(x_train_2D, y_train)\n",
        "#model.evaluate(x_validation_2D, y_validation)\n",
        "#model.evaluate(x_test_2D, y_test)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPVoPXOC08oh",
        "colab_type": "text"
      },
      "source": [
        "#### Best model and early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXJ7hjlRGSqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn2(X_train, Y_train, X_valid, Y_valid, X_test, Y_test,\n",
        "         filters, kernel_size, pool_size, GRUunits, epochs, batch_size):\n",
        "\n",
        "    model_rnn2 = create_rnn2(filters, kernel_size, pool_size, GRUunits)\n",
        "\n",
        "    # define a log dir in order to save the checkpoint file\n",
        "    logdir = os.path.join(os.curdir, \"my_logs\")\n",
        "\n",
        "    # in Keras the Eraly stopping is manage by using the callbacks argument.\n",
        "    callbacks_rnn2 = [\n",
        "        keras.callbacks.TensorBoard(logdir),\n",
        "        keras.callbacks.EarlyStopping(patience=5),\n",
        "        #Saving the checkpoints file allows to load the \"best\" model when the Early \n",
        "        #stopping detect that the generalization error degrade (after 'patience' epochs)\n",
        "        keras.callbacks.ModelCheckpoint(\"my_model_rnn2.h5\", save_best_only=True),\n",
        "    ]\n",
        "\n",
        "    history = model_rnn2.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
        "                        validation_data=(X_valid, Y_valid),\n",
        "                        callbacks=callbacks_rnn2,\n",
        "                        verbose = 1)\n",
        "\n",
        "    #model_rnn2 = keras.models.load_model(\"my_model_rnn2.h5\")\n",
        "\n",
        "    plot_loss(history)\n",
        "    plot_accuracy(history)\n",
        "\n",
        "    Y_train_pred = np.argmax(model_rnn2.predict(X_train), axis=1)\n",
        "    Y_valid_pred = np.argmax(model_rnn2.predict(X_valid), axis=1)\n",
        "    Y_test_pred = np.argmax(model_rnn2.predict(X_test), axis=1)\n",
        "    \n",
        "    # print accuracy\n",
        "    print(f'Train Accuracy: {accuracy_score(Y_train, Y_train_pred):.4f}')\n",
        "    print(f'Valid Accuracy: {accuracy_score(Y_valid, Y_valid_pred):.4f}')\n",
        "    print(f'Test  Accuracy: {accuracy_score(Y_test,  Y_test_pred):.4f}')\n",
        "    \n",
        "    return (Y_train_pred, Y_valid_pred, Y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D4SmGCp2sJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model\n",
        "filters=32\n",
        "kernel_size=5\n",
        "pool_size= 3\n",
        "GRUunits=64  #16\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = rnn2(x_train_2D, y_train,\n",
        "                                               x_validation_2D, y_validation,\n",
        "                                               x_test_2D, y_test,\n",
        "                                               filters, kernel_size, pool_size,\n",
        "                                               GRUunits, epochs, batch_size)\n",
        "\n",
        "# print_results(y_train, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFbFZeFgHQDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# best model on augmented data (hyperparameters found with grid search on augmented)\n",
        "'''\n",
        "filters = 32\n",
        "kernel_size = 5\n",
        "pool_size = 3\n",
        "GRUunits = 64\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "y_train_pred, y_valid_pred, y_test_pred = rnn2(x_train_2D_aug, y_train_aug,\n",
        "                                               x_validation_2D, y_validation,\n",
        "                                               x_test_2D, y_test,\n",
        "                                               filters, kernel_size, pool_size,\n",
        "                                               GRUunits, epochs, batch_size)\n",
        "\n",
        "# print_results(y_train_aug, y_train_pred)\n",
        "# print_results(y_validation, y_valid_pred)\n",
        "# print_results(y_test, y_test_pred)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYmJvpaG5RU9",
        "colab_type": "text"
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "* time shift\n",
        "* time masking\n",
        "* white noise addition\n",
        "* frequency masking\n",
        "* mix combinations in a function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghw3PTYlfcBU",
        "colab_type": "text"
      },
      "source": [
        "## Time shift \n",
        "Shift to left or right from 1 to 5 slots time, filling with silence (-9.21 Mel frequency). Recall that each slot corresponds to 4 msec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88z7PWb1yJk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#time shift\n",
        "idx=1006\n",
        "\n",
        "shifted=-9.21*np.ones([32,32])\n",
        "shifted[:,:29]=x_train_mat[idx][:,3:].copy()\n",
        "\n",
        "\n",
        "# Plot audio feature\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.title(f'Mel-Spectrogram of audio: {df_train.iloc[idx][\"word\"]}', fontweight='bold')\n",
        "plt.imshow(x_train_mat[idx], aspect='auto', origin='low', cmap='inferno')\n",
        "plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "plt.show()\n",
        "\n",
        "# Plot audio feature\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.title(f'Mel-Spectrogram of audio: {df_train.iloc[idx][\"word\"]}', fontweight='bold')\n",
        "plt.imshow(shifted, aspect='auto', origin='low', cmap='inferno')\n",
        "plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Umj82OhOD3",
        "colab_type": "text"
      },
      "source": [
        "## Time masking\n",
        "\n",
        "Silence a randomly selected slot of time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B1bPn5hhyid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#time masking\n",
        "idx=1006\n",
        "\n",
        "silenced=x_train_mat[idx].copy()\n",
        "silenced[:,15]=-9.21*np.ones(32)\n",
        "\n",
        "\n",
        "# Plot audio feature\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.title(f'Mel-Spectrogram of audio: {df_train.iloc[idx][\"word\"]}', fontweight='bold')\n",
        "plt.imshow(x_train_mat[idx], aspect='auto', origin='low', cmap='inferno')\n",
        "plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "plt.show()\n",
        "\n",
        "# Plot audio feature\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.title(f'Mel-Spectrogram of audio: {df_train.iloc[idx][\"word\"]}', fontweight='bold')\n",
        "plt.imshow(silenced, aspect='auto', origin='low', cmap='inferno')\n",
        "plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDuwG2wInRGr",
        "colab_type": "text"
      },
      "source": [
        "## White noise\n",
        "\n",
        "Add white noise to the sample. White noises are random samples distributed at regular intervals with mean of 0 and standard deviation of 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPYsJ97B8nGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seaborn histogram\n",
        "sns.distplot(x_train_mat.reshape(-1), hist=True, kde=False, \n",
        "             bins=int(180/5), color = 'blue',\n",
        "             hist_kws={'edgecolor':'black'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_AhNA1HvRQQ",
        "colab_type": "text"
      },
      "source": [
        "## Frequency masking\n",
        "\n",
        "Silence a randomly selected frequency band."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkr0asVgvSaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.min(x_train_mat))\n",
        "print(np.shape(x_train_mat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cnWprwzvStN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# frequency masking\n",
        "idx=406\n",
        "\n",
        "fsilenced = x_train_mat[idx].copy()\n",
        "fsilenced[9:11,:]=-9.21*np.ones((2,32))\n",
        "\n",
        "# Plot original feature\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.title(f'Mel-Spectrogram of audio: {df_train.iloc[idx][\"word\"]}', fontweight='bold')\n",
        "plt.imshow(x_train_mat[idx], aspect='auto', origin='low', cmap='inferno')\n",
        "plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "plt.show()\n",
        "\n",
        "# Plot edited feature\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.title(f'Mel-Spectrogram of audio: {df_train.iloc[idx][\"word\"]}', fontweight='bold')\n",
        "plt.imshow(fsilenced, aspect='auto', origin='low', cmap='inferno')\n",
        "plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaIdxl2VYi2T",
        "colab_type": "text"
      },
      "source": [
        "## Combinations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL-em8-CJo1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(data_mat, y, shift=True, time=True, freq=True, noise=True, noise_coeff=0.1, bern_prob = 0.8):\n",
        "  minEn = np.min(data_mat)\n",
        "  suppl = data_mat.copy()\n",
        "\n",
        "  for i in range(data_mat.shape[0]):\n",
        "\n",
        "    if time and np.random.binomial(1,bern_prob):\n",
        "      # sample time slot and its width\n",
        "      t = np.random.randint(1,3)\n",
        "      t0 = np.random.randint(0,32+1-t)\n",
        "      suppl[i][t0:t0+t,:] = minEn*np.ones((t,32))\n",
        "\n",
        "    if shift and np.random.binomial(1,bern_prob):\n",
        "      # sample direction\n",
        "      l = np.random.binomial(1,0.5)\n",
        "      # sample shift\n",
        "      s = np.random.randint(2,6)\n",
        "      if l == 1:\n",
        "        suppl[i][:,:32-s] = suppl[i][:,s:].copy()\n",
        "        suppl[i][:,32-s:] = minEn*np.ones((32,s))\n",
        "      else:\n",
        "        suppl[i][:,s:] = suppl[i][:,:32-s].copy()\n",
        "        suppl[i][:,:s] = minEn*np.ones((32,s))\n",
        "\n",
        "    if freq and np.random.binomial(1,bern_prob):\n",
        "      # sample time slot and its width\n",
        "      f = np.random.randint(1,3)\n",
        "      f0 = np.random.randint(0,32+1-f)\n",
        "      suppl[i][:,f0:f0+f] = minEn*np.ones((32,f))\n",
        "  \n",
        "  if noise and np.random.binomial(1,bern_prob):\n",
        "      noisemat = noise_coeff*np.abs(np.random.normal(0,1,1600*32*32))\n",
        "      noisemat = noisemat.reshape(1600,32,32)\n",
        "      suppl += noisemat\n",
        "\n",
        "  suppl = np.array(suppl)\n",
        "\n",
        "  augmented = np.append(data_mat,suppl,0)\n",
        "  #print(augmented.shape)\n",
        "\n",
        "  y_aug = np.append(y, y, 0)\n",
        "  #print(y_aug.shape)\n",
        "\n",
        "  augmented_flat = augmented.reshape(augmented.shape[0], -1)\n",
        "  #print(augmented_flat.shape)\n",
        "\n",
        "  '''\n",
        "  idx = np.random.randint(0,data_mat.shape[0])\n",
        "\n",
        "  # Plot audio feature\n",
        "  \n",
        "  # Plot audio feature\n",
        "  plt.figure(figsize=(5, 3))\n",
        "  plt.title(f'Mel-Spectrogram of audio: {df_train.iloc[idx][\"word\"]}', fontweight='bold')\n",
        "  plt.imshow(data_mat[idx], aspect='auto', origin='low', cmap='inferno')\n",
        "  plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "  plt.show()\n",
        "\n",
        "  # Plot audio feature\n",
        "  plt.figure(figsize=(5, 3))\n",
        "  plt.title(f'Mel-Spectrogram of edited audio: {df_train.iloc[idx][\"word\"]}', fontweight='bold')\n",
        "  plt.imshow(suppl[idx], aspect='auto', origin='low', cmap='inferno')\n",
        "  plt.grid(lw=0.4, c='w', alpha=0.4)\n",
        "  plt.show()\n",
        "  '''\n",
        "\n",
        "  return (augmented, augmented_flat, y_aug, suppl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XirUDPBtnXVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_train_aug_mat, x_train_aug, y_train_aug, x_train_new = augment(x_train_mat, y_train, freq=False, noise_coeff=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfPueezF3Lo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "from numpy import save\n",
        "save(os.path.join(DATA_BASE_FOLDER, 'x_train_aug_mat.npy'), x_train_aug_mat)\n",
        "save(os.path.join(DATA_BASE_FOLDER, 'x_train_aug.npy'), x_train_aug)\n",
        "save(os.path.join(DATA_BASE_FOLDER, 'y_train_aug.npy'), y_train_aug)\n",
        "save(os.path.join(DATA_BASE_FOLDER, 'x_train_new.npy'), x_train_new)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DqirPXuIQ5Jr"
      },
      "source": [
        "# Audio reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE6p8K4asjUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx=1005\n",
        "\n",
        "#original audio \n",
        "\n",
        "ipd.Audio(audio_train[idx], rate=SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woqe0rWesaN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#original data (128,32) -  reconstr audio\n",
        "np.exp(x_train_raw[idx]).shape\n",
        "\n",
        "S = librosa.feature.inverse.mel_to_audio(np.exp(x_train_raw[idx]), sr=SAMPLE_RATE, hop_length=HOP_LEN)\n",
        "\n",
        "ipd.Audio(S, rate=SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvPg-DdasiL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#original data (32,32) -  reconstr audio\n",
        "\n",
        "np.exp(x_train_mat[idx]).shape\n",
        "\n",
        "S = librosa.feature.inverse.mel_to_audio(np.exp(x_train_mat[idx]), sr=SAMPLE_RATE, hop_length=HOP_LEN)\n",
        "\n",
        "ipd.Audio(S, rate=SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xehhqvdSw1Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shifted data (32,32) -  reconstr audio\n",
        "\n",
        "shifted=-9.21*np.ones([32,32])\n",
        "shifted[:,:29]=x_train_mat[idx][:,3:].copy()\n",
        "\n",
        "np.exp(shifted).shape\n",
        "\n",
        "S = librosa.feature.inverse.mel_to_audio(np.exp(shifted), sr=SAMPLE_RATE, hop_length=HOP_LEN)\n",
        "\n",
        "ipd.Audio(S, rate=SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEs5AXjxZs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#time mask (32,32) -  reconstr audio\n",
        "\n",
        "silenced=x_train_mat[idx].copy()\n",
        "silenced[:,15]=-9.21*np.ones(32)\n",
        "\n",
        "np.exp(silenced).shape\n",
        "\n",
        "S = librosa.feature.inverse.mel_to_audio(np.exp(silenced), sr=SAMPLE_RATE, hop_length=HOP_LEN)\n",
        "\n",
        "ipd.Audio(S, rate=SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS3XQ06bxwK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#white noise (32,32) -  reconstr audio\n",
        "\n",
        "noisemat=0.5*np.random.normal(0,1,32*32)\n",
        "noisemat=np.abs(noisemat.reshape(32,32))\n",
        "noised=x_train_mat[idx]+noisemat\n",
        "\n",
        "np.exp(noised).shape\n",
        "\n",
        "S = librosa.feature.inverse.mel_to_audio(np.exp(noised), sr=SAMPLE_RATE, hop_length=HOP_LEN)\n",
        "\n",
        "ipd.Audio(S, rate=SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNoIdThixzYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#freq mask (32,32) -  reconstr audio\n",
        "\n",
        "fsilenced = x_train_mat[idx].copy()\n",
        "fsilenced[9:11,:]=-9.21*np.ones((2,32))\n",
        "\n",
        "np.exp(fsilenced).shape\n",
        "\n",
        "S = librosa.feature.inverse.mel_to_audio(np.exp(fsilenced), sr=SAMPLE_RATE, hop_length=HOP_LEN)\n",
        "\n",
        "ipd.Audio(S, rate=SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX5SuJLd7z8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#spy test\n",
        "\n",
        "S = librosa.feature.inverse.mel_to_audio(np.exp(x_test_mat[26]), sr=SAMPLE_RATE, hop_length=HOP_LEN)\n",
        "ipd.Audio(S, rate=SAMPLE_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIbEvmQMuyp1",
        "colab_type": "text"
      },
      "source": [
        "# Send the submission for the challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJLW5ZPouyp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################\n",
        "# Save your test prediction in y_test_pred\n",
        "##################################################\n",
        "\n",
        "#y_test_pred = None\n",
        "\n",
        "# Create submission\n",
        "submission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\n",
        "if y_test_pred is not None:\n",
        "    submission['word'] = [labels[int(y_i)] for y_i in y_test_pred]\n",
        "submission.to_csv('my_submission.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
